{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "803c6900",
   "metadata": {},
   "source": [
    "# Lab 1 Text Processing Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596edd3b",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36143923",
   "metadata": {},
   "source": [
    "Downloading Libs and Testing That They Are Working\n",
    "https://github.com/hb20007/hands-on-nltk-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "755af8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install nltk\n",
    "# ! pip install nltk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a02688",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b37a08",
   "metadata": {},
   "source": [
    "#### Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d518c73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kaishuai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download punkt package\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbbe564c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Mrkgnao!\n",
      "> the cat said loudly.\n",
      "> She blinked up out of her avid shameclosing eyes, mewing plaintively and long, showing him her milkwhite teeth.\n",
      "> He watched the dark eyeslits narrowing with greed till her eyes were green stones.\n",
      "> Then he went to the dresser, took the jug Hanlon'smilkman had just filled for him, poured warmbubbled milk on a saucer and set it slowly on the floor.— Gurrhr!\n",
      "> she cried, running to lap.\n"
     ]
    }
   ],
   "source": [
    "passage = \"Mrkgnao! the cat said loudly. She blinked up out of her avid shameclosing eyes, mewing \\\n",
    "plaintively and long, showing him her milkwhite teeth. He watched the dark eyeslits narrowing \\\n",
    "with greed till her eyes were green stones. Then he went to the dresser, took the jug Hanlon's\\\n",
    "milkman had just filled for him, poured warmbubbled milk on a saucer and set it slowly on the floor.\\\n",
    "— Gurrhr! she cried, running to lap.\"\n",
    "\n",
    "doc = nltk.sent_tokenize(passage)\n",
    "for s in doc:\n",
    "    print(\">\",s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88ef2e",
   "metadata": {},
   "source": [
    "#### Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33213124",
   "metadata": {},
   "source": [
    "There are different methods for tokenizing text into words, such as:\n",
    "1. TreebankWordTokenizer\n",
    "2. WordPunctTokenizer\n",
    "3. WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c4acbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEFAULT:  ['Mary', 'had', 'a', 'little', 'lamb', 'it', \"'s\", 'fleece', 'was', 'white', 'as', 'snow', '.']\n",
      "PUNCT  :  ['Mary', 'had', 'a', 'little', 'lamb', 'it', \"'\", 's', 'fleece', 'was', 'white', 'as', 'snow', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"Mary had a little lamb it's fleece was white as snow.\"\n",
    "\n",
    "# default Tokenization\n",
    "default_tokens = word_tokenize(sentence)   # nltk.download('punkt') for this\n",
    "\n",
    "# WordPunctTokenizer\n",
    "punct_tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "punct_tokens = punct_tokenizer.tokenize(sentence)\n",
    "\n",
    "print(\"DEFAULT: \", default_tokens)\n",
    "print(\"PUNCT  : \", punct_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fcec30",
   "metadata": {},
   "source": [
    "Exercise: use nltk.tokenize.SpaceTokenizer() to tokenize the above sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788a9055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your codes here\n",
    "# space_tokenizer = nltk.tokenize.SpaceTokenizer()\n",
    "# space_tokens = space_tokenizer.tokenize(sentence)\n",
    "\n",
    "# print(\"SPACE: \", space_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c455ad36",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa5aa8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/kaishuai/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download package\n",
    "nltk.download('averaged_perceptron_tagger', download_dir=\"your_download_dir\")  # replace \"your_download_dir\" with your download directory, the following packages are loaded in the same way\n",
    "nltk.data.path = [\"your_download_dir\"] + nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b01b4537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEFAULT:  [('Mary', 'NNP'), ('had', 'VBD'), ('a', 'DT'), ('little', 'JJ'), ('lamb', 'NN'), ('it', 'PRP'), (\"'s\", 'VBZ'), ('fleece', 'NN'), ('was', 'VBD'), ('white', 'JJ'), ('as', 'IN'), ('snow', 'NN'), ('.', '.')]\n",
      "PUNCT  :  [('Mary', 'NNP'), ('had', 'VBD'), ('a', 'DT'), ('little', 'JJ'), ('lamb', 'NN'), ('it', 'PRP'), (\"'\", \"''\"), ('s', 'JJ'), ('fleece', 'NN'), ('was', 'VBD'), ('white', 'JJ'), ('as', 'IN'), ('snow', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_default = nltk.pos_tag(default_tokens)\n",
    "print(\"DEFAULT: \", pos_default)\n",
    "\n",
    "pos_punct = nltk.pos_tag(punct_tokens)\n",
    "print(\"PUNCT  : \", pos_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c4382",
   "metadata": {},
   "source": [
    "Exercise: tage POS of the whitespace tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62a20441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your codes here\n",
    "# pos_space = nltk.pos_tag(space_tokens)\n",
    "\n",
    "# print(\"SPACE: \", pos_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3a0768",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d383806a",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d5187a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PorterStemmer: ['mari', 'had', 'a', 'littl', 'lamb', 'it', \"'s\", 'fleec', 'wa', 'white', 'as', 'snow', '.']\n",
      "LancasterStemmer: ['mary', 'had', 'a', 'littl', 'lamb', 'it', \"'s\", 'fleec', 'was', 'whit', 'as', 'snow', '.']\n",
      "SnowballStemmer: ['mari', 'had', 'a', 'littl', 'lamb', 'it', \"'s\", 'fleec', 'was', 'white', 'as', 'snow', '.']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "snowball = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "\n",
    "print(\"PorterStemmer:\", [porter.stem(t) for t in default_tokens])\n",
    "print(\"LancasterStemmer:\", [lancaster.stem(t) for t in default_tokens])\n",
    "print(\"SnowballStemmer:\", [snowball.stem(t) for t in default_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4cb0c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: When I was going into the woods I saw a bear lying asleep on the forest floor\n",
      "['when', 'I', 'wa', 'go', 'into', 'the', 'wood', 'I', 'saw', 'a', 'bear', 'lie', 'asleep', 'on', 'the', 'forest', 'floor']\n",
      "['when', 'i', 'was', 'going', 'into', 'the', 'wood', 'i', 'saw', 'a', 'bear', 'lying', 'asleep', 'on', 'the', 'forest', 'flo']\n",
      "['when', 'i', 'was', 'go', 'into', 'the', 'wood', 'i', 'saw', 'a', 'bear', 'lie', 'asleep', 'on', 'the', 'forest', 'floor']\n"
     ]
    }
   ],
   "source": [
    "# try another sentence\n",
    "sentence2 = \"When I was going into the woods I saw a bear lying asleep on the forest floor\"\n",
    "print(\"Sentence:\", sentence2)\n",
    "\n",
    "# first tokenize the sentence\n",
    "tokens2 = word_tokenize(sentence2)\n",
    "# then apply stemming\n",
    "for stemmer in [porter, lancaster, snowball]:\n",
    "    print([stemmer.stem(t) for t in tokens2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdfa0f0",
   "metadata": {},
   "source": [
    "#### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40d6c0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/kaishuai/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39a0d8ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary', 'had', 'a', 'little', 'lamb', 'it', \"'s\", 'fleece', 'wa', 'white', 'a', 'snow', '.']\n"
     ]
    }
   ],
   "source": [
    "# try word net lemmatizer\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "print([wnl.lemmatize(t) for t in default_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61582a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: When I was going into the woods I saw a bear lying asleep on the forest floor\n",
      "['When', 'I', 'wa', 'going', 'into', 'the', 'wood', 'I', 'saw', 'a', 'bear', 'lying', 'asleep', 'on', 'the', 'forest', 'floor']\n"
     ]
    }
   ],
   "source": [
    "# try another sentence\n",
    "sentence2 = \"When I was going into the woods I saw a bear lying asleep on the forest floor\"\n",
    "print(\"Sentence:\", sentence2)\n",
    "\n",
    "# first tokenize the sentence\n",
    "tokens2 = word_tokenize(sentence2)\n",
    "# then apply lemmatization\n",
    "tokens2_pos = nltk.pos_tag(tokens2)\n",
    "print([wnl.lemmatize(t) for t in tokens2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7638d7d8",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c5d5e",
   "metadata": {},
   "source": [
    "Download spaCy according to the official website: https://spacy.io/usage#quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66a1d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c38fff0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\tlemma\tpos\ttag\tdep\tshape\tis_alpha\tis_stop\n",
      "When\twhen\tADV\tWRB\tadvmod\tXxxx\tTrue\tTrue\n",
      "I\t-PRON-\tPRON\tPRP\tnsubj\tX\tTrue\tTrue\n",
      "was\tbe\tVERB\tVBD\taux\txxx\tTrue\tTrue\n",
      "going\tgo\tVERB\tVBG\tadvcl\txxxx\tTrue\tFalse\n",
      "into\tinto\tADP\tIN\tprep\txxxx\tTrue\tTrue\n",
      "the\tthe\tDET\tDT\tdet\txxx\tTrue\tTrue\n",
      "woods\twood\tNOUN\tNNS\tpobj\txxxx\tTrue\tFalse\n",
      "I\t-PRON-\tPRON\tPRP\tnsubj\tX\tTrue\tTrue\n",
      "saw\tsee\tVERB\tVBD\tROOT\txxx\tTrue\tFalse\n",
      "a\ta\tDET\tDT\tdet\tx\tTrue\tTrue\n",
      "bear\tbear\tNOUN\tNN\tnsubj\txxxx\tTrue\tFalse\n",
      "lying\tlie\tVERB\tVBG\tccomp\txxxx\tTrue\tFalse\n",
      "asleep\tasleep\tADJ\tJJ\tdobj\txxxx\tTrue\tFalse\n",
      "on\ton\tADP\tIN\tprep\txx\tTrue\tTrue\n",
      "the\tthe\tDET\tDT\tdet\txxx\tTrue\tTrue\n",
      "forest\tforest\tNOUN\tNN\tcompound\txxxx\tTrue\tFalse\n",
      "floor\tfloor\tNOUN\tNN\tpobj\txxxx\tTrue\tFalse\n",
      "?\t?\tPUNCT\t.\tpunct\t?\tFalse\tFalse\n"
     ]
    }
   ],
   "source": [
    "# try analyzing the sentence\n",
    "# all the information is available in the token object\n",
    "doc = nlp(\"When I was going into the woods I saw a bear lying asleep on the forest floor?\")\n",
    "\n",
    "print(\"text\\tlemma\\tpos\\ttag\\tdep\\tshape\\tis_alpha\\tis_stop\")\n",
    "for token in doc:\n",
    "    print(token.text, \"\\t\", token.lemma_, \"\\t\", token.pos_, \"\\t\", token.tag_, \"\\t\", token.dep_, \"\\t\", \n",
    "            token.shape_, \"\\t\", token.is_alpha, \"\\t\", token.is_stop, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f643834e",
   "metadata": {},
   "source": [
    "## Vectorization and Document Similarity Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b298ea1",
   "metadata": {},
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc58b3af",
   "metadata": {},
   "source": [
    "Content in demofile.txt: \\\n",
    "Mars is the fourth planet in our solar system. \\\n",
    "It is second-smallest planet in the Solar System after Mercury. \\\n",
    "Saturn is yellow planet.\n",
    "\n",
    "Content (query) in demofile2.txt: \\\n",
    "Mars is approximately half the diameter of Earth. \n",
    "\n",
    "Tasks: \\\n",
    "1.vectorize each sentence; \\\n",
    "2.compute the similarities between the query and each document in the demofile.txt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7372e8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mars', 'is', 'the', 'fourth', 'planet', 'in', 'our', 'solar', 'system', '.'], ['it', 'is', 'second-smallest', 'planet', 'in', 'the', 'solar', 'system', 'after', 'mercury', '.'], ['saturn', 'is', 'yellow', 'planet', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk, gensim\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "gen_docs = []\n",
    "\n",
    "# first tokenzie the sentence, which is the first step in most NLP tasks\n",
    "with open ('data/demofile.txt') as f:\n",
    "    docs=f.readlines()\n",
    "    for doc in docs:\n",
    "        gen_docs.append([w.lower() for w in word_tokenize(doc)])\n",
    "\n",
    "print(gen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6b180c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'fourth': 1, 'in': 2, 'is': 3, 'mars': 4, 'our': 5, 'planet': 6, 'solar': 7, 'system': 8, 'the': 9, 'after': 10, 'it': 11, 'mercury': 12, 'second-smallest': 13, 'saturn': 14, 'yellow': 15}\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary for all tokens in the documents\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44960b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)]\n",
      "[(0, 1), (2, 1), (3, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)]\n",
      "[(0, 1), (3, 1), (6, 1), (14, 1), (15, 1)]\n"
     ]
    }
   ],
   "source": [
    "# create a bag-of-words corpus\n",
    "corpus = []\n",
    "for doc in gen_docs:\n",
    "    corpus.append(dictionary.doc2bow(doc))\n",
    "    print(corpus[-1])\n",
    "\n",
    "# (0, 1) means that word id 0 appears once in the first document\n",
    "# (1, 1) means that word id 1 appears once in the first document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2279fb",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27058816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (3, 16)\n",
      "One-hot Vector:\n",
      " [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# simple one-hot vector representation based on the bag-of-words\n",
    "v = gensim.matutils.corpus2csc(corpus).toarray().T\n",
    "v[v > 1] = 1\n",
    "print(\"Shape:\", v.shape)  # 3 documents, 16 unique words (i.e., the size of the dictionary)\n",
    "print(\"One-hot Vector:\\n\", v)  # >0 if the word appears in the document, 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a82179",
   "metadata": {},
   "source": [
    "### Documnet Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e3b7b",
   "metadata": {},
   "source": [
    "#### Similarity based on One-Hot representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d008e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_doc ['mars', 'is', 'a', 'cold', 'desert', 'world', '.', 'it', 'is', 'half', 'the', 'size', 'of', 'the', 'earth', '.']\n",
      "[(0, 2), (3, 2), (4, 1), (9, 2), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "# load the query and tokenize it\n",
    "with open ('data/demofile2.txt') as f:\n",
    "    query = f.readlines()[0]\n",
    "    query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "print('query_doc', query_doc)\n",
    "\n",
    "# convert the query to bag-of-words\n",
    "query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "print(query_doc_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fd790be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (16,)\n",
      "One-hot Vector:\n",
      " [1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "v = gensim.matutils.corpus2csc(corpus + [query_doc_bow]).toarray().T\n",
    "v[v > 1] = 1\n",
    "corpus_v = v[:-1]\n",
    "query_v = v[-1]\n",
    "print(\"Shape:\", query_v.shape)  # 1 document, 16 unique words (i.e., the size of the dictionary)\n",
    "print(\"One-hot Vector:\\n\", query_v)  # >0 if the word appears in the document, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "520ca18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Result: [0.5656854249492379, 0.5393598899705937, 0.3999999999999999]\n"
     ]
    }
   ],
   "source": [
    "# calculate the cosine similarity\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# calculate the cosine similarity\n",
    "def cos_sim(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "# calculate the cosine similarity\n",
    "cos_similarities = [cos_sim(query_v, d) for d in corpus_v]\n",
    "print('Comparing Result:', cos_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74a3401",
   "metadata": {},
   "source": [
    "#### Similarity based on TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55b97e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fourth', 0.53), ('in', 0.2), ('mars', 0.53), ('our', 0.53), ('solar', 0.2), ('system', 0.2), ('the', 0.2)]\n",
      "[('in', 0.17), ('solar', 0.17), ('system', 0.17), ('the', 0.17), ('after', 0.47), ('it', 0.47), ('mercury', 0.47), ('second-smallest', 0.47)]\n",
      "[('saturn', 0.71), ('yellow', 0.71)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create a tf-idf model from the corpus\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "\n",
    "for doc in tf_idf[corpus]:\n",
    "    print([(dictionary[id], np.around(freq, decimals=2)) for id, freq in doc])\n",
    "\n",
    "# ('fourth', 0.53) means that the word 'fourth' has a tf-idf score of 0.53 in the first document\n",
    "# ('saturn', 0.71) means that the word 'saturn' has a tf-idf score of 0.71 in the third document\n",
    "# Some tokens may appear in most documents, leading to their TF-IDF values being too low to display in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c659817e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity index with 3 documents in 0 shards (stored under gensim)\n"
     ]
    }
   ],
   "source": [
    "# building the index\n",
    "sims = gensim.similarities.Similarity('gensim', tf_idf[corpus], num_features=len(dictionary))\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8084f2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_doc ['mars', 'is', 'a', 'cold', 'desert', 'world', '.', 'it', 'is', 'half', 'the', 'size', 'of', 'the', 'earth', '.']\n",
      "[(0, 2), (3, 2), (4, 1), (9, 2), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "# load the query and tokenize it\n",
    "with open ('data/demofile2.txt') as f:\n",
    "    query = f.readlines()[0]\n",
    "    query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "print('query_doc',query_doc)\n",
    "\n",
    "# convert the query to bag-of-words\n",
    "query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "print(query_doc_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e059a962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Result: [0.42364514 0.37414625 0.        ]\n"
     ]
    }
   ],
   "source": [
    "# perform a similarity query against the corpus\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "\n",
    "# print document similarity\n",
    "print('Comparing Result:', sims[query_doc_tf_idf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168e4db1",
   "metadata": {},
   "source": [
    "## Classification using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f515fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC News Training DataFrame:\n",
      "      ArticleId                                               Text  Category\n",
      "0         1833  worldcom ex-boss launches defence lawyers defe...  business\n",
      "1          154  german business confidence slides german busin...  business\n",
      "2         1101  bbc poll indicates economic gloom citizens in ...  business\n",
      "3         1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
      "4          917  enron bosses in $168m payout eighteen former e...  business\n",
      "..         ...                                                ...       ...\n",
      "994       1647  van nistelrooy set to return manchester united...     sport\n",
      "995       1827  yelling takes cardiff hat-trick european cross...     sport\n",
      "996       2187  battered dollar hits another low the dollar ha...  business\n",
      "997       1897  turkey turns on the economic charm three years...  business\n",
      "998       1795  robinson ready for difficult task england coac...     sport\n",
      "\n",
      "[999 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the training data\n",
    "train_df = pd.read_csv('data/BBC_News_Train.csv', header=0)\n",
    "print('BBC News Training DataFrame:\\n', train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9bd1a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# tokenize the news\n",
    "news_docs = []\n",
    "for sent in train_df.Text.tolist():\n",
    "    news_docs.append([w.lower() for w in word_tokenize(sent)])\n",
    "\n",
    "# get labels (here, we simplify the category into two classes: business and not-business)\n",
    "label_text = train_df.Category.tolist()\n",
    "labels = [1 if label == 'business' else 0 for label in label_text]\n",
    "print('Labels:', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "063fbe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training word2vec model on all news documents. This may take a moment.\n",
    "word2vec = gensim.models.Word2Vec(news_docs,\n",
    "                        size = 300,\n",
    "                        window = 8,\n",
    "                        min_count = 5,\n",
    "                        sg = 0,\n",
    "                        alpha = 0.025,\n",
    "                        iter=10,\n",
    "                        batch_words = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ffd0fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (999, 300)\n"
     ]
    }
   ],
   "source": [
    "# create vector for each sentence based on word2vec\n",
    "data = []\n",
    "\n",
    "# simply add the word vectors in the sentence as the sentence vector\n",
    "for i in range(len(news_docs)):\n",
    "    tmp = np.zeros([300])\n",
    "    for w in news_docs[i]:\n",
    "        if w in word2vec.wv:\n",
    "            tmp = tmp + word2vec.wv[w]\n",
    "\n",
    "    data.append(tmp)\n",
    "data = np.array(data)\n",
    "print('Shape:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "970af9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c133b690",
   "metadata": {},
   "source": [
    "### Train a Binary classifier using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62bf918d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a XGBoost model to classsify\n",
    "# ! pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model_XGB = XGBClassifier()\n",
    "model_XGB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d0d9379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not Business       0.95      0.98      0.96       149\n",
      "    Business       0.93      0.84      0.89        51\n",
      "\n",
      "    accuracy                           0.94       200\n",
      "   macro avg       0.94      0.91      0.93       200\n",
      "weighted avg       0.94      0.94      0.94       200\n",
      "\n",
      "AUC = 0.98499802605606\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "y_prob = model_XGB.predict_proba(X_test)[:,1]\n",
    "\n",
    "#y_prob is the probability of the class being 1\n",
    "#you can get the class predictions by thresholding the probabilities\n",
    "#for example, here, we use 0.5 as the threshold, i.e., if the probability is greater than 0.5, we predict the class as 1, otherwise 0\n",
    "\n",
    "# when threshold=0.5, the precision, recall, and F1 score\n",
    "y_pred = np.where(y_prob > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\n",
    "auc_roc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "target_names = ['Not Business', 'Business']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(\"AUC =\", auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f00b816",
   "metadata": {},
   "source": [
    "Practice:\n",
    "1. you can try to construct other models such as linear reagression model, tree-based models, SVM model, RNN models, simple MLP models, etc.;\n",
    "2. you can try other processing operations such as removing stopwords, removing punctuation, removing numbers, etc.;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0d0624",
   "metadata": {},
   "source": [
    "## Classification using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065a68a9",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba77a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4986b036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "102\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab = tokenizer.get_vocab() # a dictionary\n",
    "\n",
    "# special tokens and ids\n",
    "print(vocab['[CLS]'])\n",
    "print(vocab['[SEP]'])\n",
    "print(vocab['[PAD]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acba37e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokenization: ['[CLS]', 'he', 'remains', 'characteristic', '##ally', 'confident', 'and', 'optimistic', '.']\n",
      "Sentence ids: [101, 2002, 3464, 8281, 3973, 9657, 1998, 21931, 1012]\n"
     ]
    }
   ],
   "source": [
    "# try to tokenize a sentence\n",
    "sent1 = \"[CLS] He remains characteristically confident and optimistic.\"\n",
    "encoding1 = tokenizer.tokenize(sent1)\n",
    "sent_ids1 = tokenizer.convert_tokens_to_ids(encoding1)\n",
    "print('Sentence tokenization:', encoding1)\n",
    "print('Sentence ids:',sent_ids1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccf3a409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokenization: ['he', 'remains', 'characteristic', '##ally', 'confident', 'and', 'optimistic', '.', '[SEP]', 'therefore', ',', 'i', 'believe', 'he', 'will', 'finally', 'achieve', 'success', '.', '[PAD]']\n",
      "Sentence ids: [2002, 3464, 8281, 3973, 9657, 1998, 21931, 1012, 102, 3568, 1010, 1045, 2903, 2002, 2097, 2633, 6162, 3112, 1012, 0]\n"
     ]
    }
   ],
   "source": [
    "# try to tokenize a sentence\n",
    "sent2 = \"He remains characteristically confident and optimistic. \\\n",
    "        [SEP] Therefore, I believe he will finally achieve success. [PAD]\"\n",
    "encoding2 = tokenizer.tokenize(sent2)\n",
    "sent_ids2 = tokenizer.convert_tokens_to_ids(encoding2)\n",
    "print('Sentence tokenization:', encoding2)\n",
    "print('Sentence ids:',sent_ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77b82c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 101, 2002, 3464, 8281, 3973, 9657, 1998, 21931, 1012, 102, 2002, 3464, 8281, 3973, 9657, 1998, 21931, 1012, 102, 3568, 1010, 1045, 2903, 2002, 2097, 2633, 6162, 3112, 1012, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# try to encode a pair of sentences\n",
    "# the tokenizer will automatically add [CLS] and [SEP] tokens\n",
    "# token_type_ids will be used to distinguish the two sentences (input as 0 or 1, they will be converted to another embedding vector)\n",
    "sent_encoding = tokenizer.encode_plus(sent1, sent2)\n",
    "print(sent_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8df2b6",
   "metadata": {},
   "source": [
    "### Configuration & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd648682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# parameters for the model\n",
    "configuration = BertConfig()\n",
    "model = BertModel(configuration)\n",
    "configuration = model.config\n",
    "print(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4675e59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: {'input_ids': tensor([[  101,  2002,  3464,  8281,  3973,  9657,  1998, 21931,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Last hidden states: tensor([[[-0.3684, -0.0749, -0.3237,  ..., -0.0464,  0.3429,  0.2990],\n",
      "         [-0.3090, -0.2400, -0.3401,  ..., -0.0900,  0.8401, -0.6425],\n",
      "         [-0.0244,  0.1629, -0.3323,  ..., -0.1590, -0.3468,  0.1759],\n",
      "         ...,\n",
      "         [-0.2457,  0.1194,  0.1172,  ...,  0.0415, -0.2971, -0.5022],\n",
      "         [ 0.6263, -0.0320, -0.3789,  ...,  0.2608, -0.4490, -0.3570],\n",
      "         [ 0.4926,  0.0245, -0.1460,  ...,  0.3488, -0.5853, -0.4121]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "Last hidden states size: torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# load the pre-trained model \n",
    "# (you can also download the model from the link: https://huggingface.co/google-bert/bert-base-uncased, \n",
    "# and then load it from the local directory)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "inputs = tokenizer(\"He remains characteristically confident and optimistic.\",\n",
    "                   return_tensors=\"pt\")\n",
    "print('Inputs:', inputs)\n",
    "\n",
    "# the input is a dictionary with keys: input_ids, token_type_ids, attention_mask\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs[0]\n",
    "print('Last hidden states:', last_hidden_states)\n",
    "print('Last hidden states size:',last_hidden_states.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04b77df",
   "metadata": {},
   "source": [
    "### Train a Multi-Class Sentence Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22d9d2f",
   "metadata": {},
   "source": [
    "BBC News Train.csv includes dataset for news classification data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a4b0f2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC News Training DataFrame:\n",
      "      ArticleId                                               Text  Category\n",
      "0         1833  worldcom ex-boss launches defence lawyers defe...  business\n",
      "1          154  german business confidence slides german busin...  business\n",
      "2         1101  bbc poll indicates economic gloom citizens in ...  business\n",
      "3         1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
      "4          917  enron bosses in $168m payout eighteen former e...  business\n",
      "..         ...                                                ...       ...\n",
      "994       1647  van nistelrooy set to return manchester united...     sport\n",
      "995       1827  yelling takes cardiff hat-trick european cross...     sport\n",
      "996       2187  battered dollar hits another low the dollar ha...  business\n",
      "997       1897  turkey turns on the economic charm three years...  business\n",
      "998       1795  robinson ready for difficult task england coac...     sport\n",
      "\n",
      "[999 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the training data\n",
    "train_df = pd.read_csv('data/BBC_News_Train.csv', header=0)\n",
    "print('BBC News Training DataFrame:\\n', train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ead008a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: tensor([0, 0, 0, 3, 0, 4, 1, 2, 0, 2, 4, 2, 0, 0, 1, 1, 2, 1, 1, 3, 1, 2, 1, 1,\n",
      "        3, 0, 3, 1, 4, 0, 3, 2, 0, 4, 1, 2, 1, 1, 1, 3, 3, 1, 1, 3, 2, 2, 3, 3,\n",
      "        3, 0])\n"
     ]
    }
   ],
   "source": [
    "# get labels\n",
    "label_text = train_df.Category.tolist()[:50]\n",
    "label_type = set(label_text)\n",
    "label2id = {label: id for id, label in enumerate(label_type)}\n",
    "labels = [label2id[label] for label in label_text]\n",
    "labels = torch.tensor(labels)\n",
    "print('Labels:', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31b6ab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness. cynthia cooper worldcom s ex-head of internal accounting alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of\n",
      "Token ids: tensor([  101,  2088,  9006,  4654,  1011,  5795, 18989,  4721,  9559,  6984,\n",
      "         2280,  2088,  9006,  2708, 15941,  1041, 29325,  2015,  2114,  1037,\n",
      "         6046,  1997,  9861,  5571,  2031,  2170,  1037,  2194, 13300, 16558,\n",
      "        25114,  2004,  2037,  2034,  7409,  1012, 15809,  6201,  2088,  9006,\n",
      "         1055,  4654,  1011,  2132,  1997,  4722,  9529, 22333,  5501,  2000,\n",
      "        12052,  9529,  6078,  2012,  1996,  2149, 18126,  2015,  5016,  1999,\n",
      "         2526,  1012,  2014, 16234,  2419,  2000,  1996,  7859,  1997,  1996,\n",
      "         3813,  2206,  1996,  5456,  1997,  2019,  1002,  2340, 24700,  1006,\n",
      "        27813,  1012,  1021, 24700,  1007,  9529,  9861,  1012,  2720,  1041,\n",
      "        29325,  2015,  2038, 12254,  2025,  5905,  2000,  5571,  1997,  9861,\n",
      "         1998,   102])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaishuai/.conda/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_masks = [], []\n",
    "\n",
    "# 50 sentences for example, you can try more in your experiments\n",
    "for sent in train_df.Text[:50]:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        ' '.join(sent.split()[:100]),    # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 102,           # Pad & truncate all sentences. (control the length so that you can construct the input as matrix)\n",
    "                        pad_to_max_length = True,       #(control the length so that you can construct the input as matrix)\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "print('Original: ', ' '.join(train_df.Text[0].split()[:60]))\n",
    "print('Token ids:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "743dafb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   45 training samples\n",
      "    5 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset into training and validation by randomly selecting samples\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "764976da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 4\n",
    "\n",
    "# create the dataloader to load the data automatically in batches\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially (the original order).\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddbb5922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (5, 768)\n",
      "classifier.bias                                                 (5,)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "#BertForSequenceClassification is used for multi-calss classification\n",
    "#if you want to use a model for binary classification, you can use other BERT series models or construct a new model based on BertModel\n",
    "\n",
    "cls_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = len(label_type), # The number of output labels.\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# use GPU\n",
    "cls_model.cuda()\n",
    "params = list(cls_model.named_parameters())\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5cc1fbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaishuai/.conda/envs/py38/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# set the optimizer\n",
    "optimizer = AdamW(cls_model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0,  # you can change this value for your experiments\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac660ffd",
   "metadata": {},
   "source": [
    "Following two blocks provide two help function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e17837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d0c9b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "716c604f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 3/12 [00:00<00:00, 22.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch     4  of     12.    Elapsed: 0:00:00.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 9/12 [00:00<00:00, 26.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch     8  of     12.    Elapsed: 0:00:00.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 26.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 1.07\n",
      "  Training epcoh took: 0:00:00\n",
      "\n",
      "Running Validation...\n",
      "tensor(1.2061, device='cuda:0')\n",
      "tensor(1.1778, device='cuda:0')\n",
      "  Accuracy: 0.88\n",
      "  Validation Loss: 1.19\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 3/12 [00:00<00:00, 28.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch     4  of     12.    Elapsed: 0:00:00.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 9/12 [00:00<00:00, 28.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch     8  of     12.    Elapsed: 0:00:00.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 28.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 1.19\n",
      "  Training epcoh took: 0:00:00\n",
      "\n",
      "Running Validation...\n",
      "tensor(1.2061, device='cuda:0')\n",
      "tensor(1.1778, device='cuda:0')\n",
      "  Accuracy: 0.88\n",
      "  Validation Loss: 1.19\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 4/12 [00:00<00:00, 31.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch     4  of     12.    Elapsed: 0:00:00.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 8/12 [00:00<00:00, 28.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch     8  of     12.    Elapsed: 0:00:00.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 28.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 1.11\n",
      "  Training epcoh took: 0:00:00\n",
      "\n",
      "Running Validation...\n",
      "tensor(1.2061, device='cuda:0')\n",
      "tensor(1.1778, device='cuda:0')\n",
      "  Accuracy: 0.88\n",
      "  Validation Loss: 1.19\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 4/12 [00:00<00:00, 31.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch     4  of     12.    Elapsed: 0:00:00.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 8/12 [00:00<00:00, 30.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch     8  of     12.    Elapsed: 0:00:00.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 30.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 1.14\n",
      "  Training epcoh took: 0:00:00\n",
      "\n",
      "Running Validation...\n",
      "tensor(1.2061, device='cuda:0')\n",
      "tensor(1.1778, device='cuda:0')\n",
      "  Accuracy: 0.88\n",
      "  Validation Loss: 1.19\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 4/12 [00:00<00:00, 31.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch     4  of     12.    Elapsed: 0:00:00.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 8/12 [00:00<00:00, 30.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch     8  of     12.    Elapsed: 0:00:00.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 30.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 1.13\n",
      "  Training epcoh took: 0:00:00\n",
      "\n",
      "Running Validation...\n",
      "tensor(1.2061, device='cuda:0')\n",
      "tensor(1.1778, device='cuda:0')\n",
      "  Accuracy: 0.88\n",
      "  Validation Loss: 1.19\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:00:02 (h:mm:ss)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "epochs = 5\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    \n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    cls_model.train()\n",
    "    \n",
    "    # For each batch of training data...\n",
    "    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 4 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        b_input_ids = batch[0].to('cuda')\n",
    "        b_input_mask = batch[1].to('cuda')\n",
    "        b_labels = batch[2].to('cuda')\n",
    "        \n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        cls_model.zero_grad()   \n",
    "        outputs = cls_model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss = total_train_loss + loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(cls_model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    \n",
    "    \n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    cls_model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to('cuda')\n",
    "        b_input_mask = batch[1].to('cuda')\n",
    "        b_labels = batch[2].to('cuda')\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "\n",
    "            outputs = cls_model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels\n",
    "                           )\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "            print(loss)\n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.cpu().numpy()\n",
    "\n",
    "        #logits is the probability of each class\n",
    "        #multi-class predictions are obtained by taking the argmax of the logits\n",
    "        #Notice: binary classification is different from multi-class classification when get the predictions\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abea229a",
   "metadata": {},
   "source": [
    "### Multiple Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8c386f",
   "metadata": {},
   "source": [
    "Another example based on BERT for multiple choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74e08e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMultipleChoice\n",
    "\n",
    "MC_model = BertForMultipleChoice.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c25a3993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.6706, grad_fn=<NllLossBackward0>)\n",
      "Labels: tensor([0])\n",
      "Logits: tensor([[0.4332, 0.3876]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "choice0 = \"It is eaten with a fork and a knife.\"\n",
    "choice1 = \"It is eaten while held in the hand.\"\n",
    "labels = torch.tensor(0).unsqueeze(0)\n",
    "encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=\"pt\", padding=True)\n",
    "outputs = MC_model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "print('Loss:', loss)\n",
    "print('Labels:', labels)\n",
    "print('Logits:', logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee35f9b",
   "metadata": {},
   "source": [
    "Practice:\n",
    "1. Have a try with SWAG_MC_Train.csv;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b36872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
